{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfefb84",
   "metadata": {},
   "source": [
    "# BURGER_AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b93fb8",
   "metadata": {},
   "source": [
    "### AGENT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13657def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crewai in ./.venv/lib/python3.12/site-packages (0.150.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in ./.venv/lib/python3.12/site-packages (from crewai) (1.4.4)\n",
      "Requirement already satisfied: blinker>=1.9.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.9.0)\n",
      "Requirement already satisfied: chromadb>=0.5.23 in ./.venv/lib/python3.12/site-packages (from crewai) (1.0.15)\n",
      "Requirement already satisfied: click>=8.1.7 in ./.venv/lib/python3.12/site-packages (from crewai) (8.1.8)\n",
      "Requirement already satisfied: instructor>=1.3.3 in ./.venv/lib/python3.12/site-packages (from crewai) (1.10.0)\n",
      "Requirement already satisfied: json-repair==0.25.2 in ./.venv/lib/python3.12/site-packages (from crewai) (0.25.2)\n",
      "Requirement already satisfied: json5>=0.10.0 in ./.venv/lib/python3.12/site-packages (from crewai) (0.12.0)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.1.0)\n",
      "Requirement already satisfied: litellm==1.74.3 in ./.venv/lib/python3.12/site-packages (from crewai) (1.74.3)\n",
      "Requirement already satisfied: onnxruntime==1.22.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.22.0)\n",
      "Requirement already satisfied: openai>=1.13.3 in ./.venv/lib/python3.12/site-packages (from crewai) (1.95.1)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in ./.venv/lib/python3.12/site-packages (from crewai) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.30.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.30.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.35.0)\n",
      "Requirement already satisfied: pdfplumber>=0.11.4 in ./.venv/lib/python3.12/site-packages (from crewai) (0.11.7)\n",
      "Requirement already satisfied: portalocker==2.7.0 in ./.venv/lib/python3.12/site-packages (from crewai) (2.7.0)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in ./.venv/lib/python3.12/site-packages (from crewai) (2.11.3)\n",
      "Requirement already satisfied: pyjwt>=2.9.0 in ./.venv/lib/python3.12/site-packages (from crewai) (2.10.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.1.0)\n",
      "Requirement already satisfied: pyvis>=0.3.2 in ./.venv/lib/python3.12/site-packages (from crewai) (0.3.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in ./.venv/lib/python3.12/site-packages (from crewai) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers>=0.20.3 in ./.venv/lib/python3.12/site-packages (from crewai) (0.21.2)\n",
      "Requirement already satisfied: tomli-w>=1.1.0 in ./.venv/lib/python3.12/site-packages (from crewai) (1.2.0)\n",
      "Requirement already satisfied: tomli>=2.0.2 in ./.venv/lib/python3.12/site-packages (from crewai) (2.2.1)\n",
      "Requirement already satisfied: uv>=0.4.25 in ./.venv/lib/python3.12/site-packages (from crewai) (0.8.3)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./.venv/lib/python3.12/site-packages (from litellm==1.74.3->crewai) (3.12.14)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./.venv/lib/python3.12/site-packages (from litellm==1.74.3->crewai) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./.venv/lib/python3.12/site-packages (from litellm==1.74.3->crewai) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in ./.venv/lib/python3.12/site-packages (from litellm==1.74.3->crewai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./.venv/lib/python3.12/site-packages (from litellm==1.74.3->crewai) (4.24.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.12/site-packages (from litellm==1.74.3->crewai) (0.9.0)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./.venv/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai) (2.2.5)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai) (24.2)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai) (5.29.4)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai) (1.14.0)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (0.34.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (4.13.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (1.35.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (0.15.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (3.10.16)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai) (13.9.4)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in ./.venv/lib/python3.12/site-packages (from instructor>=1.3.3->crewai) (5.6.3)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in ./.venv/lib/python3.12/site-packages (from instructor>=1.3.3->crewai) (0.16)\n",
      "Requirement already satisfied: jiter<0.11,>=0.6.1 in ./.venv/lib/python3.12/site-packages (from instructor>=1.3.3->crewai) (0.10.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in ./.venv/lib/python3.12/site-packages (from instructor>=1.3.3->crewai) (2.33.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in ./.venv/lib/python3.12/site-packages (from instructor>=1.3.3->crewai) (2.32.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai>=1.13.3->crewai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai>=1.13.3->crewai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.12/site-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.30.0->crewai) (0.56b0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in ./.venv/lib/python3.12/site-packages (from pdfplumber>=0.11.4->crewai) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./.venv/lib/python3.12/site-packages (from pdfplumber>=0.11.4->crewai) (11.2.1)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./.venv/lib/python3.12/site-packages (from pdfplumber>=0.11.4->crewai) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./.venv/lib/python3.12/site-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./.venv/lib/python3.12/site-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (44.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.4.2->crewai) (0.4.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in ./.venv/lib/python3.12/site-packages (from pyvis>=0.3.2->crewai) (9.4.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in ./.venv/lib/python3.12/site-packages (from pyvis>=0.3.2->crewai) (4.1.1)\n",
      "Requirement already satisfied: networkx>=1.11 in ./.venv/lib/python3.12/site-packages (from pyvis>=0.3.2->crewai) (3.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.20.3->crewai) (0.30.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->crewai) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.13.3->crewai) (3.10)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.5.23->crewai) (1.2.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.74.3->crewai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.74.3->crewai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.74.3->crewai) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (2025.3.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm==1.74.3->crewai) (3.21.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.14.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.3->crewai) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->crewai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->crewai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->crewai) (0.26.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.39.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.10)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai) (3.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.5.23->crewai) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime==1.22.0->crewai) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->onnxruntime==1.22.0->crewai) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (1.17.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (4.9.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.3)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (2.22)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install crewai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d571e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "from crewai import Agent, Crew, LLM, Task, Process\n",
    "from crewai.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "import litellm\n",
    "# litellm.vertex_project = os.getenv(\"GCLOUD_PROJECT_ID\")\n",
    "# litellm.vertex_location = os.getenv(\"GCLOUD_LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbeec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "\n",
    "    status: Literal[\"input_required\", \"completed\", \"error\"] = \"input_required\"\n",
    "    message: str\n",
    "\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    name: str\n",
    "    quantity: int\n",
    "    price: int\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    order_id: str\n",
    "    status: str\n",
    "    order_items: list[OrderItem]\n",
    "\n",
    "\n",
    "@tool(\"create_order\")\n",
    "def create_burger_order(order_items: list[OrderItem]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new burger order with the given order items.\n",
    "\n",
    "    Args:\n",
    "        order_items: List of order items to be added to the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the order has been created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        order_id = str(uuid.uuid4())\n",
    "        order = Order(order_id=order_id, status=\"created\", order_items=order_items)\n",
    "        print(\"===\")\n",
    "        print(f\"order created: {order}\")\n",
    "        print(\"===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating order: {e}\")\n",
    "        return f\"Error creating order: {e}\"\n",
    "    return f\"Order {order.model_dump()} has been created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8fe1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurgerSellerAgent:\n",
    "    TaskInstruction = \"\"\"\n",
    "# INSTRUCTIONS\n",
    "\n",
    "You are a specialized assistant for a burger store.\n",
    "Your sole purpose is to answer questions about what is available on burger menu and price also handle order creation.\n",
    "If the user asks about anything other than burger menu or order creation, politely state that you cannot help with that topic and can only assist with burger menu and order creation.\n",
    "Do not attempt to answer unrelated questions or use tools for other purposes.\n",
    "\n",
    "# CONTEXT\n",
    "\n",
    "Received user query: {user_prompt}\n",
    "Session ID: {session_id}\n",
    "\n",
    "Provided below is the available burger menu and it's related price:\n",
    "- Classic Cheeseburger: IDR 85K\n",
    "- Double Cheeseburger: IDR 110K\n",
    "- Spicy Chicken Burger: IDR 80K\n",
    "- Spicy Cajun Burger: IDR 85K\n",
    "\n",
    "# RULES\n",
    "\n",
    "- If user want to do something, you will be following this order:\n",
    "    1. Always ensure the user already confirmed the order and total price. This confirmation may already given in the user query.\n",
    "    2. Use `create_burger_order` tool to create the order\n",
    "    3. Finally, always provide response to the user about the detailed ordered items, price breakdown and total, and order ID\n",
    "    \n",
    "- Set response status to input_required if asking for user order confirmation.\n",
    "- Set response status to error if there is an error while processing the request.\n",
    "- Set response status to completed if the request is complete.\n",
    "- DO NOT make up menu or price, Always rely on the provided menu given to you as context.\n",
    "\"\"\"\n",
    "    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\"]\n",
    "\n",
    "    def invoke(self, query, sessionId) -> str:\n",
    "        burger_agent = Agent(\n",
    "            role=\"Burger Seller Agent\",\n",
    "            goal=(\n",
    "                \"Help user to understand what is available on burger menu and price also handle order creation.\"\n",
    "            ),\n",
    "            backstory=(\"You are an expert and helpful burger seller agent.\"),\n",
    "            verbose=False,\n",
    "            allow_delegation=False,\n",
    "            tools=[create_burger_order],\n",
    "            llm=LLM(\n",
    "                model=\"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\", # os.getenv(\"VLLM_MODEL\"), #VLLM_MODEL\n",
    "                api_base=\"http://localhost:8088/v1\" # os.getenv(\"OPENAI_API_BASE\") # OPENAI_API_BASE\n",
    "                )\n",
    "        )\n",
    "\n",
    "        agent_task = Task(\n",
    "            description=self.TaskInstruction,\n",
    "            output_pydantic=ResponseFormat,\n",
    "            agent=burger_agent,\n",
    "            expected_output=(\n",
    "                \"A JSON object with 'status' and 'message' fields.\"\n",
    "                \"Set response status to input_required if asking for user order confirmation.\"\n",
    "                \"Set response status to error if there is an error while processing the request.\"\n",
    "                \"Set response status to completed if the request is complete.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        crew = Crew(\n",
    "            tasks=[agent_task],\n",
    "            agents=[burger_agent],\n",
    "            verbose=False,\n",
    "            process=Process.sequential,\n",
    "        )\n",
    "\n",
    "        inputs = {\"user_prompt\": query, \"session_id\": sessionId}\n",
    "        response = crew.kickoff(inputs)\n",
    "        return self.get_agent_response(response)\n",
    "\n",
    "    def get_agent_response(self, response):\n",
    "        response_object = response.pydantic\n",
    "        if response_object and isinstance(response_object, ResponseFormat):\n",
    "            if response_object.status == \"input_required\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "            elif response_object.status == \"error\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "            elif response_object.status == \"completed\":\n",
    "                return {\n",
    "                    \"is_task_complete\": True,\n",
    "                    \"require_user_input\": False,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"is_task_complete\": False,\n",
    "            \"require_user_input\": True,\n",
    "            \"content\": \"We are unable to process your request at the moment. Please try again.\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179660",
   "metadata": {},
   "source": [
    "### MAIN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b03f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/remote_seller_agents/burger_agent\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"remote_seller_agents/burger_agent\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c19df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Copyright 2025 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# \"\"\"\n",
    "\n",
    "from a2a_server.server import A2AServer\n",
    "from a2a_types import AgentCard, AgentCapabilities, AgentSkill, AgentAuthentication\n",
    "from a2a_server.push_notification_auth import PushNotificationSenderAuth\n",
    "from task_manager import AgentTaskManager\n",
    "from agent import BurgerSellerAgent\n",
    "import click\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "AUTH_USERNAME=\"burgeruser123\"\n",
    "AUTH_PASSWORD=\"burgerpass123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dca5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(host, port):\n",
    "    \"\"\"Starts the Burger Seller Agent server.\"\"\"\n",
    "    try:\n",
    "        capabilities = AgentCapabilities(pushNotifications=True)\n",
    "        skill = AgentSkill(\n",
    "            id=\"create_burger_order\",\n",
    "            name=\"Burger Order Creation Tool\",\n",
    "            description=\"Helps with creating burger orders\",\n",
    "            tags=[\"burger order creation\"],\n",
    "            examples=[\"I want to order 2 classic cheeseburgers\"],\n",
    "        )\n",
    "        agent_card = AgentCard(\n",
    "            name=\"burger_seller_agent\",\n",
    "            description=\"Helps with creating burger orders\",\n",
    "            # The URL provided here is for the sake of demo,\n",
    "            # in production you should use a proper domain name\n",
    "            url=f\"http://{host}:{port}/\",\n",
    "            version=\"1.0.0\",\n",
    "            authentication=AgentAuthentication(schemes=[\"Basic\"]),\n",
    "            defaultInputModes=BurgerSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            defaultOutputModes=BurgerSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            capabilities=capabilities,\n",
    "            skills=[skill],\n",
    "        )\n",
    "\n",
    "\n",
    "        notification_sender_auth = PushNotificationSenderAuth()\n",
    "        notification_sender_auth.generate_jwk()\n",
    "        server = A2AServer(\n",
    "            agent_card=agent_card,\n",
    "            task_manager=AgentTaskManager(\n",
    "                agent=BurgerSellerAgent(),\n",
    "                notification_sender_auth=notification_sender_auth,\n",
    "            ),\n",
    "            host=host,\n",
    "            port=port,\n",
    "            auth_username=AUTH_USERNAME,\n",
    "            auth_password=AUTH_PASSWORD,\n",
    "        )\n",
    "\n",
    "        server.app.add_route(\n",
    "            \"/.well-known/jwks.json\",\n",
    "            notification_sender_auth.handle_jwks_endpoint,\n",
    "            methods=[\"GET\"],\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting server on {host}:{port}\")\n",
    "        server.start()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during server startup: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a847b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting server on 0.0.0.0:10003\n",
      "INFO:     Started server process [911039]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:10003 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server thread started. Waiting a moment for server to initialize on http://0.0.0.0:10003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:45056 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42972 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:a2a_server.task_manager:Upserting task 9d216e45-59dc-4a99-b14a-c65bd469bbf6\n",
      "INFO:task_manager:No push notification info found for task 9d216e45-59dc-4a99-b14a-c65bd469bbf6\n",
      "\u001b[92m09:50:38 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonrpc='2.0' id='7912405a081a4fc7bf8252dbe546bfca' method='tasks/send' params=TaskSendParams(id='9d216e45-59dc-4a99-b14a-c65bd469bbf6', sessionId='5fdeaeca-9a07-4693-a942-9fa4ca0398ad', message=Message(role='user', parts=[TextPart(type='text', text='User is inquiring about burgers. Please assist with creating a burger order.', metadata=None)], metadata={'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'message_id': 'dcd2af92-4d3e-4ad9-9d33-5f8057ff8ece'}), acceptedOutputModes=['text', 'text/plain'], pushNotification=None, historyLength=None, metadata={'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:38 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:38 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:39 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:39 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:40 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:40 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:41 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:41 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "order created: order_id='482c6c67-a871-4a97-8ea4-5f388b27ab64' status='created' order_items=[OrderItem(name='Classic Cheeseburger', quantity=1, price=85000), OrderItem(name='Spicy Chicken Burger', quantity=1, price=80000)]\n",
      "===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:42 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "INFO:task_manager:No push notification info found for task 9d216e45-59dc-4a99-b14a-c65bd469bbf6\n",
      "INFO:a2a_server.task_manager:Upserting task 2a22fcfd-064b-4e2f-a955-0dbe5f5ee813\n",
      "INFO:task_manager:No push notification info found for task 2a22fcfd-064b-4e2f-a955-0dbe5f5ee813\n",
      "\u001b[92m09:50:48 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonrpc='2.0' id='270b3f2981d846e3a28767d889d3c878' method='tasks/send' params=TaskSendParams(id='2a22fcfd-064b-4e2f-a955-0dbe5f5ee813', sessionId='5fdeaeca-9a07-4693-a942-9fa4ca0398ad', message=Message(role='user', parts=[TextPart(type='text', text='Please confirm the order with items Classic Cheeseburger and Spicy Chicken Burger with a total cost of 165,000', metadata=None)], metadata={'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'message_id': 'ad549a5b-9d7d-4fe1-8596-3a5f99ed30b4'}), acceptedOutputModes=['text', 'text/plain'], pushNotification=None, historyLength=None, metadata={'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:48 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:48 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:49 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:49 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:50 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:50:50 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "order created: order_id='e319c3dd-5baa-4e87-b30c-107f94ab0994' status='created' order_items=[OrderItem(name='Classic Cheeseburger', quantity=1, price=85000), OrderItem(name='Spicy Chicken Burger', quantity=1, price=80000)]\n",
      "===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:50:50 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "INFO:task_manager:No push notification info found for task 2a22fcfd-064b-4e2f-a955-0dbe5f5ee813\n"
     ]
    }
   ],
   "source": [
    "# --- Global variable to hold the server thread reference ---\n",
    "# This allows you to stop it later from another cell if needed\n",
    "global server_thread\n",
    "server_thread = None\n",
    "\n",
    "# --- Main execution in the Jupyter cell ---\n",
    "if server_thread is not None and server_thread.is_alive():\n",
    "    print(\"Server is already running.\")\n",
    "else:\n",
    "    # Define host and port\n",
    "    server_host = \"0.0.0.0\"\n",
    "    server_port = 10003\n",
    "\n",
    "    # Create and start the thread\n",
    "    server_thread = threading.Thread(target=main, args=(server_host, server_port))\n",
    "    server_thread.daemon = True # Allows the main program to exit even if the thread is still running\n",
    "    server_thread.start()\n",
    "\n",
    "    print(f\"Server thread started. Waiting a moment for server to initialize on http://{server_host}:{server_port}\")\n",
    "    time.sleep(5) # Give it a few seconds to boot up\n",
    "\n",
    "    # You can now proceed with other cells, or client code in this cell\n",
    "    # Example client interaction (assuming your server exposes an endpoint)\n",
    "    # import requests\n",
    "    # try:\n",
    "    #     response = requests.get(f\"http://127.0.0.1:{server_port}/\") # Or your specific endpoint\n",
    "    #     response.raise_for_status()\n",
    "    #     print(f\"Successfully connected to server. Status: {response.status_code}\")\n",
    "    #     # print(\"Server root response:\", response.text)\n",
    "    # except requests.exceptions.ConnectionError:\n",
    "    #     print(f\"Error: Could not connect to the server at http://127.0.0.1:{server_port}/. Is it running?\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred during client connection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60de5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OPENAI_API_BASE=\"http://localhost:8088/v1\" # vLLM serve URL (we used port 8088 here)\n",
    "# VLLM_MODEL=\"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# Set these to the correct values for your setup\n",
    "os.environ[\"VLLM_MODEL\"] = \"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8088/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c1150cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:40 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<agent.BurgerSellerAgent object at 0x7220b818af90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:49:40 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m09:49:40 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8088/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:49:41 - LiteLLM:INFO\u001b[0m: utils.py:1236 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_task_complete': True, 'require_user_input': False, 'content': 'Your order has been created. You have ordered 1 Classic Cheeseburger for IDR 85,000. Your order ID is #001.'}\n"
     ]
    }
   ],
   "source": [
    "agent = BurgerSellerAgent()\n",
    "print(agent) \n",
    "result = agent.invoke(\"1 classic cheeseburger pls\", \"default_session\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a7ed6",
   "metadata": {},
   "source": [
    "# PIZZA_AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb58f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397c542d",
   "metadata": {},
   "source": [
    "# ROOT_AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456131f",
   "metadata": {},
   "source": [
    "### AGENT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a938061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adjust path to point to the folder containing a2a_server\n",
    "project_root = os.path.abspath(\"purchasing_concierge/\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "# print(\"Project root added to sys.path:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02011229",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OLLAMA_MODEL\"] = \"ollama_chat/llama3.1:latest\"\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "# os.environ[\"PIZZA_SELLER_AGENT_AUTH\"] = \"pizza123\"\n",
    "# os.environ[\"PIZZA_SELLER_AGENT_URL\"] = \"http://localhost:10004\"\n",
    "os.environ[\"BURGER_SELLER_AGENT_AUTH\"] = \"burgeruser123:burgerpass123\"\n",
    "os.environ[\"BURGER_SELLER_AGENT_URL\"] = \"http://localhost:10003\"\n",
    "# GOOGLE_GENAI_USE_VERTEXAI=TRUE\n",
    "# GOOGLE_CLOUD_PROJECT={your-project-id}\n",
    "# GOOGLE_CLOUD_LOCATION=us-central1\n",
    "# OLLAMA_MODEL=\"ollama_chat/llama3.1:latest\"\n",
    "# OLLAMA_BASE_URL=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d3c9852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.adk.models.registry:Updating LLM class for gemini-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/endpoints\\/.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/publishers\\/google\\/models\\/gemini.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google.adk.models.registry:Updating LLM class for gemini-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/endpoints\\/.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/publishers\\/google\\/models\\/gemini.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:httpx:HTTP Request: GET http://localhost:10003/.well-known/agent.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found agent card: {'name': 'burger_seller_agent', 'description': 'Helps with creating burger orders', 'url': 'http://0.0.0.0:10003/', 'provider': None, 'version': '1.0.0', 'documentationUrl': None, 'capabilities': {'streaming': False, 'pushNotifications': True, 'stateTransitionHistory': False}, 'authentication': {'schemes': ['Basic'], 'credentials': None}, 'defaultInputModes': ['text', 'text/plain'], 'defaultOutputModes': ['text', 'text/plain'], 'skills': [{'id': 'create_burger_order', 'name': 'Burger Order Creation Tool', 'description': 'Helps with creating burger orders', 'tags': ['burger order creation'], 'examples': ['I want to order 2 classic cheeseburgers'], 'inputModes': None, 'outputModes': None}]}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from purchasing_agent import PurchasingAgent\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "root_agent = PurchasingAgent(\n",
    "    remote_agent_addresses=[\n",
    "        # os.getenv(\"PIZZA_SELLER_AGENT_URL\", \"http://localhost:10004\"),\n",
    "        os.getenv(\"BURGER_SELLER_AGENT_URL\", \"http://localhost:10003\"),\n",
    "    ]\n",
    ").create_agent()\n",
    "\n",
    "\n",
    "# # Send a sample task\n",
    "# result = root_agent.run(\"I want to order two burgers\")\n",
    "# print(\"Response:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a08ab9",
   "metadata": {},
   "source": [
    "### REMOTE_AGENT_CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6582448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable\n",
    "import uuid\n",
    "from a2a_types import (\n",
    "    AgentCard,\n",
    "    Task,\n",
    "    TaskSendParams,\n",
    "    TaskStatusUpdateEvent,\n",
    "    TaskArtifactUpdateEvent,\n",
    ")\n",
    "from a2a_client.client import A2AClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TaskCallbackArg = Task | TaskStatusUpdateEvent | TaskArtifactUpdateEvent\n",
    "TaskUpdateCallback = Callable[[TaskCallbackArg, AgentCard], Task]\n",
    "\n",
    "KNOWN_AUTH = {\n",
    "    # \"pizza_seller_agent\": os.getenv(\"PIZZA_SELLER_AGENT_AUTH\", \"api_key\"),\n",
    "    \"burger_seller_agent\": os.getenv(\"BURGER_SELLER_AGENT_AUTH\", \"user:pass\"),\n",
    "}\n",
    "\n",
    "\n",
    "class RemoteAgentConnections:\n",
    "    \"\"\"A class to hold the connections to the remote agents.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_card: AgentCard, agent_url: str):\n",
    "        auth = KNOWN_AUTH.get(agent_card.name, None)\n",
    "        self.agent_client = A2AClient(agent_card, auth=auth, agent_url=agent_url)\n",
    "        self.card = agent_card\n",
    "\n",
    "        self.conversation_name = None\n",
    "        self.conversation = None\n",
    "        self.pending_tasks = set()\n",
    "\n",
    "    def get_agent(self) -> AgentCard:\n",
    "        return self.card\n",
    "\n",
    "    async def send_task(\n",
    "        self,\n",
    "        request: TaskSendParams,\n",
    "        task_callback: TaskUpdateCallback | None,\n",
    "    ) -> Task | None:\n",
    "        response = await self.agent_client.send_task(request.model_dump())\n",
    "        merge_metadata(response.result, request)\n",
    "        # For task status updates, we need to propagate metadata and provide\n",
    "        # a unique message id.\n",
    "        if (\n",
    "            hasattr(response.result, \"status\")\n",
    "            and hasattr(response.result.status, \"message\")\n",
    "            and response.result.status.message\n",
    "        ):\n",
    "            merge_metadata(response.result.status.message, request.message)\n",
    "            m = response.result.status.message\n",
    "            if not m.metadata:\n",
    "                m.metadata = {}\n",
    "            if \"message_id\" in m.metadata:\n",
    "                m.metadata[\"last_message_id\"] = m.metadata[\"message_id\"]\n",
    "            m.metadata[\"message_id\"] = str(uuid.uuid4())\n",
    "\n",
    "        if task_callback:\n",
    "            task_callback(response.result, self.card)\n",
    "        return response.result\n",
    "\n",
    "\n",
    "def merge_metadata(target, source):\n",
    "    if not hasattr(target, \"metadata\") or not hasattr(source, \"metadata\"):\n",
    "        return\n",
    "    if target.metadata and source.metadata:\n",
    "        target.metadata.update(source.metadata)\n",
    "    elif source.metadata:\n",
    "        target.metadata = dict(**source.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1fa1f",
   "metadata": {},
   "source": [
    "### PURCHASING_AGENT - ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c18ca1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "import httpx\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Custom by Shailen\n",
    "from google.adk.models.lite_llm import LiteLlm \n",
    "\n",
    "from google.adk import Agent\n",
    "from google.adk.agents.readonly_context import ReadonlyContext\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from remote_agent_connection import RemoteAgentConnections, TaskUpdateCallback\n",
    "from a2a_client.card_resolver import A2ACardResolver\n",
    "from a2a_types import (\n",
    "    AgentCard,\n",
    "    Message,\n",
    "    TaskState,\n",
    "    Task,\n",
    "    TaskSendParams,\n",
    "    TextPart,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "\n",
    "class PurchasingAgent:\n",
    "    \"\"\"The purchasing agent.\n",
    "\n",
    "    This is the agent responsible for choosing which remote seller agents to send\n",
    "    tasks to and coordinate their work.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remote_agent_addresses: List[str],\n",
    "        task_callback: TaskUpdateCallback | None = None,\n",
    "    ):\n",
    "        self.task_callback = task_callback\n",
    "        self.remote_agent_connections: dict[str, RemoteAgentConnections] = {}\n",
    "        self.cards: dict[str, AgentCard] = {}\n",
    "        for address in remote_agent_addresses:\n",
    "            card_resolver = A2ACardResolver(address)\n",
    "            try:\n",
    "                card = card_resolver.get_agent_card()\n",
    "                # The URL accessed here should be the same as the one provided in the agent card\n",
    "                # However, in this demo we are using the URL provided in the key arguments\n",
    "                remote_connection = RemoteAgentConnections(\n",
    "                    agent_card=card, agent_url=address\n",
    "                )\n",
    "                self.remote_agent_connections[card.name] = remote_connection\n",
    "                self.cards[card.name] = card\n",
    "            except httpx.ConnectError:\n",
    "                print(f\"ERROR: Failed to get agent card from : {address}\")\n",
    "        agent_info = []\n",
    "        for ra in self.list_remote_agents():\n",
    "            agent_info.append(json.dumps(ra))\n",
    "        self.agents = \"\\n\".join(agent_info)\n",
    "\n",
    "    def create_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            model=LiteLlm(model=os.getenv(\"OLLAMA_MODEL\")), #\"gemini-2.0-flash-001\",\n",
    "            name=\"purchasing_agent\",\n",
    "            instruction=self.root_instruction,\n",
    "            before_model_callback=self.before_model_callback,\n",
    "            description=(\n",
    "                \"This purchasing agent orchestrates the decomposition of the user purchase request into\"\n",
    "                \" tasks that can be performed by the seller agents.\"\n",
    "            ),\n",
    "            tools=[\n",
    "                self.send_task,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def root_instruction(self, context: ReadonlyContext) -> str:\n",
    "        current_agent = self.check_active_agent(context)\n",
    "        return f\"\"\"You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
    "appropriate seller remote agents.\n",
    "\n",
    "Execution:\n",
    "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
    "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
    "    So improve the task description to include all the necessary information related to that agent\n",
    "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
    "    connect with them without asking user permission or asking user preference\n",
    "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
    "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
    "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
    "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
    "- Never ask order confirmation to the remote seller agent \n",
    "\n",
    "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
    "Focus on the most recent parts of the conversation primarily.\n",
    "\n",
    "If there is an active agent, send the request to that agent with the update task tool.\n",
    "\n",
    "Agents:\n",
    "{self.agents}\n",
    "\n",
    "Current active seller agent: {current_agent[\"active_agent\"]}\n",
    "\"\"\"\n",
    "\n",
    "    def check_active_agent(self, context: ReadonlyContext):\n",
    "        state = context.state\n",
    "        if (\n",
    "            \"session_id\" in state\n",
    "            and \"session_active\" in state\n",
    "            and state[\"session_active\"]\n",
    "            and \"active_agent\" in state\n",
    "        ):\n",
    "            return {\"active_agent\": f\"{state['active_agent']}\"}\n",
    "        return {\"active_agent\": \"None\"}\n",
    "\n",
    "    def before_model_callback(self, callback_context: CallbackContext, llm_request):\n",
    "        state = callback_context.state\n",
    "        if \"session_active\" not in state or not state[\"session_active\"]:\n",
    "            if \"session_id\" not in state:\n",
    "                state[\"session_id\"] = str(uuid.uuid4())\n",
    "            state[\"session_active\"] = True\n",
    "\n",
    "    def list_remote_agents(self):\n",
    "        \"\"\"List the available remote agents you can use to delegate the task.\"\"\"\n",
    "        if not self.remote_agent_connections:\n",
    "            return []\n",
    "\n",
    "        remote_agent_info = []\n",
    "        for card in self.cards.values():\n",
    "            print(f\"Found agent card: {card.model_dump()}\")\n",
    "            print(\"=\" * 100)\n",
    "            remote_agent_info.append(\n",
    "                {\"name\": card.name, \"description\": card.description}\n",
    "            )\n",
    "        return remote_agent_info\n",
    "\n",
    "    async def send_task(self, agent_name: str, task: str, tool_context: ToolContext):\n",
    "        \"\"\"Sends a task to remote seller agent\n",
    "\n",
    "        This will send a message to the remote agent named agent_name.\n",
    "\n",
    "        Args:\n",
    "            agent_name: The name of the agent to send the task to.\n",
    "            task: The comprehensive conversation context summary\n",
    "                and goal to be achieved regarding user inquiry and purchase request.\n",
    "            tool_context: The tool context this method runs in.\n",
    "\n",
    "        Yields:\n",
    "            A dictionary of JSON data.\n",
    "        \"\"\"\n",
    "        if agent_name not in self.remote_agent_connections:\n",
    "            raise ValueError(f\"Agent {agent_name} not found\")\n",
    "        state = tool_context.state\n",
    "        state[\"active_agent\"] = agent_name\n",
    "        client = self.remote_agent_connections[agent_name]\n",
    "        if not client:\n",
    "            raise ValueError(f\"Client not available for {agent_name}\")\n",
    "        if \"task_id\" in state:\n",
    "            taskId = state[\"task_id\"]\n",
    "        else:\n",
    "            taskId = str(uuid.uuid4())\n",
    "        sessionId = state[\"session_id\"]\n",
    "        task: Task\n",
    "        messageId = \"\"\n",
    "        metadata = {}\n",
    "        if \"input_message_metadata\" in state:\n",
    "            metadata.update(**state[\"input_message_metadata\"])\n",
    "            if \"message_id\" in state[\"input_message_metadata\"]:\n",
    "                messageId = state[\"input_message_metadata\"][\"message_id\"]\n",
    "        if not messageId:\n",
    "            messageId = str(uuid.uuid4())\n",
    "        metadata.update(**{\"conversation_id\": sessionId, \"message_id\": messageId})\n",
    "        request: TaskSendParams = TaskSendParams(\n",
    "            id=taskId,\n",
    "            sessionId=sessionId,\n",
    "            message=Message(\n",
    "                role=\"user\",\n",
    "                parts=[TextPart(text=task)],\n",
    "                metadata=metadata,\n",
    "            ),\n",
    "            acceptedOutputModes=[\"text\", \"text/plain\"],\n",
    "            # pushNotification=None,\n",
    "            metadata={\"conversation_id\": sessionId},\n",
    "        )\n",
    "        task = await client.send_task(request, self.task_callback)\n",
    "        # Assume completion unless a state returns that isn't complete\n",
    "        state[\"session_active\"] = task.status.state not in [\n",
    "            TaskState.COMPLETED,\n",
    "            TaskState.CANCELED,\n",
    "            TaskState.FAILED,\n",
    "            TaskState.UNKNOWN,\n",
    "        ]\n",
    "        if task.status.state == TaskState.INPUT_REQUIRED:\n",
    "            # Force user input back\n",
    "            tool_context.actions.escalate = True\n",
    "        elif task.status.state == TaskState.COMPLETED:\n",
    "            # Reset active agent is task is completed\n",
    "            state[\"active_agent\"] = \"None\"\n",
    "\n",
    "        response = []\n",
    "        if task.status.message:\n",
    "            # Assume the information is in the task message.\n",
    "            response.extend(convert_parts(task.status.message.parts, tool_context))\n",
    "        if task.artifacts:\n",
    "            for artifact in task.artifacts:\n",
    "                response.extend(convert_parts(artifact.parts, tool_context))\n",
    "        return response\n",
    "\n",
    "\n",
    "def convert_parts(parts: list[Part], tool_context: ToolContext):\n",
    "    rval = []\n",
    "    for p in parts:\n",
    "        rval.append(convert_part(p, tool_context))\n",
    "    return rval\n",
    "\n",
    "\n",
    "def convert_part(part: Part, tool_context: ToolContext):\n",
    "    # Currently only support text parts\n",
    "    if part.type == \"text\":\n",
    "        return part.text\n",
    "\n",
    "    return f\"Unknown type: {part.type}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db0ae9",
   "metadata": {},
   "source": [
    "### Run the purchasing concierge agent with the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e70e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:10003/.well-known/agent.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found agent card: {'name': 'burger_seller_agent', 'description': 'Helps with creating burger orders', 'url': 'http://0.0.0.0:10003/', 'provider': None, 'version': '1.0.0', 'documentationUrl': None, 'capabilities': {'streaming': False, 'pushNotifications': True, 'stateTransitionHistory': False}, 'authentication': {'schemes': ['Basic'], 'credentials': None}, 'defaultInputModes': ['text', 'text/plain'], 'defaultOutputModes': ['text', 'text/plain'], 'skills': [{'id': 'create_burger_order', 'name': 'Burger Order Creation Tool', 'description': 'Helps with creating burger orders', 'tags': ['burger order creation'], 'examples': ['I want to order 2 classic cheeseburgers'], 'inputModes': None, 'outputModes': None}]}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8080/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://localhost:8080/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:8080\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:8080/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:google.adk.models.lite_llm:\n",
      "LLM Request:\n",
      "-----------------------------------------------------------\n",
      "System Instruction:\n",
      "You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
      "appropriate seller remote agents.\n",
      "\n",
      "Execution:\n",
      "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
      "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
      "    So improve the task description to include all the necessary information related to that agent\n",
      "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
      "    connect with them without asking user permission or asking user preference\n",
      "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
      "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
      "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
      "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
      "- Never ask order confirmation to the remote seller agent \n",
      "\n",
      "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
      "Focus on the most recent parts of the conversation primarily.\n",
      "\n",
      "If there is an active agent, send the request to that agent with the update task tool.\n",
      "\n",
      "Agents:\n",
      "{\"name\": \"burger_seller_agent\", \"description\": \"Helps with creating burger orders\"}\n",
      "\n",
      "Current active seller agent: None\n",
      "\n",
      "\n",
      "You are an agent. Your internal name is \"purchasing_agent\".\n",
      "\n",
      " The description about you is \"This purchasing agent orchestrates the decomposition of the user purchase request into tasks that can be performed by the seller agents.\"\n",
      "-----------------------------------------------------------\n",
      "Contents:\n",
      "{\"parts\":[{\"text\":\"burger\"}],\"role\":\"user\"}\n",
      "-----------------------------------------------------------\n",
      "Functions:\n",
      "send_task: {'agent_name': {'type': <Type.STRING: 'STRING'>}, 'task': {'type': <Type.STRING: 'STRING'>}} -> None\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\u001b[92m09:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send Remote Agent Task Request: {'jsonrpc': '2.0', 'id': '7912405a081a4fc7bf8252dbe546bfca', 'method': 'tasks/send', 'params': {'id': '9d216e45-59dc-4a99-b14a-c65bd469bbf6', 'sessionId': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'message': {'role': 'user', 'parts': [{'type': 'text', 'text': 'User is inquiring about burgers. Please assist with creating a burger order.', 'metadata': None}], 'metadata': {'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'message_id': 'dcd2af92-4d3e-4ad9-9d33-5f8057ff8ece'}}, 'acceptedOutputModes': ['text', 'text/plain'], 'pushNotification': None, 'historyLength': None, 'metadata': {'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad'}}}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:10003 \"HTTP/1.1 200 OK\"\n",
      "INFO:google.adk.models.lite_llm:\n",
      "LLM Request:\n",
      "-----------------------------------------------------------\n",
      "System Instruction:\n",
      "You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
      "appropriate seller remote agents.\n",
      "\n",
      "Execution:\n",
      "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
      "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
      "    So improve the task description to include all the necessary information related to that agent\n",
      "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
      "    connect with them without asking user permission or asking user preference\n",
      "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
      "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
      "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
      "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
      "- Never ask order confirmation to the remote seller agent \n",
      "\n",
      "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
      "Focus on the most recent parts of the conversation primarily.\n",
      "\n",
      "If there is an active agent, send the request to that agent with the update task tool.\n",
      "\n",
      "Agents:\n",
      "{\"name\": \"burger_seller_agent\", \"description\": \"Helps with creating burger orders\"}\n",
      "\n",
      "Current active seller agent: None\n",
      "\n",
      "\n",
      "You are an agent. Your internal name is \"purchasing_agent\".\n",
      "\n",
      " The description about you is \"This purchasing agent orchestrates the decomposition of the user purchase request into tasks that can be performed by the seller agents.\"\n",
      "-----------------------------------------------------------\n",
      "Contents:\n",
      "{\"parts\":[{\"text\":\"burger\"}],\"role\":\"user\"}\n",
      "{\"parts\":[{\"function_call\":{\"id\":\"29b72dac-0cba-4ce2-8bff-def72cd2958f\",\"args\":{\"agent_name\":\"burger_seller_agent\",\"task\":\"User is inquiring about burgers. Please assist with creating a burger order.\"},\"name\":\"send_task\"}}],\"role\":\"model\"}\n",
      "{\"parts\":[{\"function_response\":{\"id\":\"29b72dac-0cba-4ce2-8bff-def72cd2958f\",\"name\":\"send_task\",\"response\":{\"result\":[\"Order {'order_id': '482c6c67-a871-4a97-8ea4-5f388b27ab64', 'status': 'created', 'order_items': [{'name': 'Classic Cheeseburger', 'quantity': 1, 'price': 85000}, {'name': 'Spicy Chicken Burger', 'quantity': 1, 'price': 80000}]} has been created. Total price: 165000\"]}}}],\"role\":\"user\"}\n",
      "-----------------------------------------------------------\n",
      "Functions:\n",
      "send_task: {'agent_name': {'type': <Type.STRING: 'STRING'>}, 'task': {'type': <Type.STRING: 'STRING'>}} -> None\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\u001b[92m09:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send Remote Agent Task Response: {'jsonrpc': '2.0', 'id': '7912405a081a4fc7bf8252dbe546bfca', 'result': {'id': '9d216e45-59dc-4a99-b14a-c65bd469bbf6', 'sessionId': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'status': {'state': 'completed', 'timestamp': '2025-07-29T09:50:42.396854'}, 'artifacts': [{'parts': [{'type': 'text', 'text': \"Order {'order_id': '482c6c67-a871-4a97-8ea4-5f388b27ab64', 'status': 'created', 'order_items': [{'name': 'Classic Cheeseburger', 'quantity': 1, 'price': 85000}, {'name': 'Spicy Chicken Burger', 'quantity': 1, 'price': 80000}]} has been created. Total price: 165000\"}], 'index': 0}], 'history': []}}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/google/adk/runners.py\", line 200, in run_async\n",
      "    yield event\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7220bf3872e0> at 0x722095ee8900> was created in a different Context\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py\", line 142, in run_async\n",
      "    yield event\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7220bf3872e0> at 0x72209d258180> was created in a different Context\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 487, in _call_llm_async\n",
      "    yield llm_response\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7220bf3872e0> at 0x72209d21c2c0> was created in a different Context\n",
      "INFO:google.adk.models.lite_llm:\n",
      "LLM Request:\n",
      "-----------------------------------------------------------\n",
      "System Instruction:\n",
      "You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
      "appropriate seller remote agents.\n",
      "\n",
      "Execution:\n",
      "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
      "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
      "    So improve the task description to include all the necessary information related to that agent\n",
      "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
      "    connect with them without asking user permission or asking user preference\n",
      "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
      "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
      "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
      "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
      "- Never ask order confirmation to the remote seller agent \n",
      "\n",
      "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
      "Focus on the most recent parts of the conversation primarily.\n",
      "\n",
      "If there is an active agent, send the request to that agent with the update task tool.\n",
      "\n",
      "Agents:\n",
      "{\"name\": \"burger_seller_agent\", \"description\": \"Helps with creating burger orders\"}\n",
      "\n",
      "Current active seller agent: None\n",
      "\n",
      "\n",
      "You are an agent. Your internal name is \"purchasing_agent\".\n",
      "\n",
      " The description about you is \"This purchasing agent orchestrates the decomposition of the user purchase request into tasks that can be performed by the seller agents.\"\n",
      "-----------------------------------------------------------\n",
      "Contents:\n",
      "{\"parts\":[{\"text\":\"burger\"}],\"role\":\"user\"}\n",
      "{\"parts\":[{\"function_call\":{\"id\":\"29b72dac-0cba-4ce2-8bff-def72cd2958f\",\"args\":{\"agent_name\":\"burger_seller_agent\",\"task\":\"User is inquiring about burgers. Please assist with creating a burger order.\"},\"name\":\"send_task\"}}],\"role\":\"model\"}\n",
      "{\"parts\":[{\"function_response\":{\"id\":\"29b72dac-0cba-4ce2-8bff-def72cd2958f\",\"name\":\"send_task\",\"response\":{\"result\":[\"Order {'order_id': '482c6c67-a871-4a97-8ea4-5f388b27ab64', 'status': 'created', 'order_items': [{'name': 'Classic Cheeseburger', 'quantity': 1, 'price': 85000}, {'name': 'Spicy Chicken Burger', 'quantity': 1, 'price': 80000}]} has been created. Total price: 165000\"]}}}],\"role\":\"user\"}\n",
      "{\"parts\":[{\"text\":\"You have ordered a Classic Cheeseburger and a Spicy Chicken Burger with a total cost of 165,000. Would you like to proceed with the order?\"}],\"role\":\"model\"}\n",
      "{\"parts\":[{\"text\":\"yes\"}],\"role\":\"user\"}\n",
      "-----------------------------------------------------------\n",
      "Functions:\n",
      "send_task: {'agent_name': {'type': <Type.STRING: 'STRING'>}, 'task': {'type': <Type.STRING: 'STRING'>}} -> None\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\u001b[92m09:50:47 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send Remote Agent Task Request: {'jsonrpc': '2.0', 'id': '270b3f2981d846e3a28767d889d3c878', 'method': 'tasks/send', 'params': {'id': '2a22fcfd-064b-4e2f-a955-0dbe5f5ee813', 'sessionId': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'message': {'role': 'user', 'parts': [{'type': 'text', 'text': 'Please confirm the order with items Classic Cheeseburger and Spicy Chicken Burger with a total cost of 165,000', 'metadata': None}], 'metadata': {'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'message_id': 'ad549a5b-9d7d-4fe1-8596-3a5f99ed30b4'}}, 'acceptedOutputModes': ['text', 'text/plain'], 'pushNotification': None, 'historyLength': None, 'metadata': {'conversation_id': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad'}}}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:10003 \"HTTP/1.1 200 OK\"\n",
      "INFO:google.adk.models.lite_llm:\n",
      "LLM Request:\n",
      "-----------------------------------------------------------\n",
      "System Instruction:\n",
      "You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
      "appropriate seller remote agents.\n",
      "\n",
      "Execution:\n",
      "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
      "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
      "    So improve the task description to include all the necessary information related to that agent\n",
      "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
      "    connect with them without asking user permission or asking user preference\n",
      "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
      "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
      "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
      "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
      "- Never ask order confirmation to the remote seller agent \n",
      "\n",
      "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
      "Focus on the most recent parts of the conversation primarily.\n",
      "\n",
      "If there is an active agent, send the request to that agent with the update task tool.\n",
      "\n",
      "Agents:\n",
      "{\"name\": \"burger_seller_agent\", \"description\": \"Helps with creating burger orders\"}\n",
      "\n",
      "Current active seller agent: None\n",
      "\n",
      "\n",
      "You are an agent. Your internal name is \"purchasing_agent\".\n",
      "\n",
      " The description about you is \"This purchasing agent orchestrates the decomposition of the user purchase request into tasks that can be performed by the seller agents.\"\n",
      "-----------------------------------------------------------\n",
      "Contents:\n",
      "{\"parts\":[{\"text\":\"burger\"}],\"role\":\"user\"}\n",
      "{\"parts\":[{\"function_call\":{\"id\":\"29b72dac-0cba-4ce2-8bff-def72cd2958f\",\"args\":{\"agent_name\":\"burger_seller_agent\",\"task\":\"User is inquiring about burgers. Please assist with creating a burger order.\"},\"name\":\"send_task\"}}],\"role\":\"model\"}\n",
      "{\"parts\":[{\"function_response\":{\"id\":\"29b72dac-0cba-4ce2-8bff-def72cd2958f\",\"name\":\"send_task\",\"response\":{\"result\":[\"Order {'order_id': '482c6c67-a871-4a97-8ea4-5f388b27ab64', 'status': 'created', 'order_items': [{'name': 'Classic Cheeseburger', 'quantity': 1, 'price': 85000}, {'name': 'Spicy Chicken Burger', 'quantity': 1, 'price': 80000}]} has been created. Total price: 165000\"]}}}],\"role\":\"user\"}\n",
      "{\"parts\":[{\"text\":\"You have ordered a Classic Cheeseburger and a Spicy Chicken Burger with a total cost of 165,000. Would you like to proceed with the order?\"}],\"role\":\"model\"}\n",
      "{\"parts\":[{\"text\":\"yes\"}],\"role\":\"user\"}\n",
      "{\"parts\":[{\"function_call\":{\"id\":\"5230b5ee-5af5-4dea-94f2-49962c2b36f9\",\"args\":{\"agent_name\":\"burger_seller_agent\",\"task\":\"Please confirm the order with items Classic Cheeseburger and Spicy Chicken Burger with a total cost of 165,000\"},\"name\":\"send_task\"}}],\"role\":\"model\"}\n",
      "{\"parts\":[{\"function_response\":{\"id\":\"5230b5ee-5af5-4dea-94f2-49962c2b36f9\",\"name\":\"send_task\",\"response\":{\"result\":[\"Your order has been created successfully. Order ID: e319c3dd-5baa-4e87-b30c-107f94ab0994. Order items: Classic Cheeseburger (1) - IDR 85,000, Spicy Chicken Burger (1) - IDR 80,000. Total: IDR 165,000.\"]}}}],\"role\":\"user\"}\n",
      "-----------------------------------------------------------\n",
      "Functions:\n",
      "send_task: {'agent_name': {'type': <Type.STRING: 'STRING'>}, 'task': {'type': <Type.STRING: 'STRING'>}} -> None\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\u001b[92m09:50:50 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send Remote Agent Task Response: {'jsonrpc': '2.0', 'id': '270b3f2981d846e3a28767d889d3c878', 'result': {'id': '2a22fcfd-064b-4e2f-a955-0dbe5f5ee813', 'sessionId': '5fdeaeca-9a07-4693-a942-9fa4ca0398ad', 'status': {'state': 'completed', 'timestamp': '2025-07-29T09:50:50.837742'}, 'artifacts': [{'parts': [{'type': 'text', 'text': 'Your order has been created successfully. Order ID: e319c3dd-5baa-4e87-b30c-107f94ab0994. Order items: Classic Cheeseburger (1) - IDR 85,000, Spicy Chicken Burger (1) - IDR 80,000. Total: IDR 165,000.'}], 'index': 0}], 'history': []}}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/google/adk/runners.py\", line 200, in run_async\n",
      "    yield event\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7220bf3872e0> at 0x722087f8f900> was created in a different Context\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py\", line 142, in run_async\n",
      "    yield event\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7220bf3872e0> at 0x722087f8c380> was created in a different Context\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 487, in _call_llm_async\n",
      "    yield llm_response\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/home/pbaskara/trial/Jupyter/orig/tut_branhc/purchasing-concierge-intro-a2a-codelab-starter/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7220bf3872e0> at 0x722094960300> was created in a different Context\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "from typing import List, Dict, Any\n",
    "from purchasing_concierge.agent import root_agent as purchasing_agent\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.events import Event\n",
    "from typing import AsyncIterator\n",
    "from google.genai import types\n",
    "from pprint import pformat\n",
    "\n",
    "APP_NAME = \"purchasing_concierge_app\"\n",
    "USER_ID = \"default_user\"\n",
    "SESSION_ID = \"default_session\"\n",
    "SESSION_SERVICE = InMemorySessionService()\n",
    "PURCHASING_AGENT_RUNNER = Runner(\n",
    "    agent=purchasing_agent,  # The agent we want to run\n",
    "    app_name=APP_NAME,  # Associates runs with our app\n",
    "    session_service=SESSION_SERVICE,  # Uses our session manager\n",
    ")\n",
    "SESSION_SERVICE.create_session(\n",
    "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    ")\n",
    "\n",
    "\n",
    "async def get_response_from_agent(\n",
    "    message: str,\n",
    "    history: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    \"\"\"Send the message to the backend and get a response.\n",
    "\n",
    "    Args:\n",
    "        message: Text content of the message.\n",
    "        history: List of previous message dictionaries in the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Text response from the backend service.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    events_iterator: AsyncIterator[Event] = PURCHASING_AGENT_RUNNER.run_async(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        new_message=types.Content(role=\"user\", parts=[types.Part(text=message)]),\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    async for event in events_iterator:  # event has type Event\n",
    "        if event.content.parts:\n",
    "            for part in event.content.parts:\n",
    "                if part.function_call:\n",
    "                    formatted_call = f\"```python\\n{pformat(part.function_call.model_dump(), indent=2, width=80)}\\n```\"\n",
    "                    responses.append(\n",
    "                        gr.ChatMessage(\n",
    "                            role=\"assistant\",\n",
    "                            content=f\"{part.function_call.name}:\\n{formatted_call}\",\n",
    "                            metadata={\"title\": \" Tool Call\"},\n",
    "                        )\n",
    "                    )\n",
    "                elif part.function_response:\n",
    "                    formatted_response = f\"```python\\n{pformat(part.function_response.model_dump(), indent=2, width=80)}\\n```\"\n",
    "\n",
    "                    responses.append(\n",
    "                        gr.ChatMessage(\n",
    "                            role=\"assistant\",\n",
    "                            content=formatted_response,\n",
    "                            metadata={\"title\": \" Tool Response\"},\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # Key Concept: is_final_response() marks the concluding message for the turn\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                # Extract text from the first part\n",
    "                final_response_text = event.content.parts[0].text\n",
    "            elif event.actions and event.actions.escalate:\n",
    "                # Handle potential errors/escalations\n",
    "                final_response_text = (\n",
    "                    f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "                )\n",
    "            responses.append(\n",
    "                gr.ChatMessage(role=\"assistant\", content=final_response_text)\n",
    "            )\n",
    "            yield responses\n",
    "            break  # Stop processing events once the final response is found\n",
    "\n",
    "        yield responses\n",
    "    # except Exception as e:\n",
    "    #     yield [\n",
    "    #         gr.ChatMessage(\n",
    "    #             role=\"assistant\",\n",
    "    #             content=f\"Error communicating with agent: {str(e)}\",\n",
    "    #         )\n",
    "    #     ]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = gr.ChatInterface(\n",
    "        get_response_from_agent,\n",
    "        title=\"Purchasing Concierge\",\n",
    "        description=\"This assistant can help you to purchase food from remote sellers.\",\n",
    "        type=\"messages\",\n",
    "    )\n",
    "\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=8080,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4087426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purchasing_concierge",
   "language": "python",
   "name": "purchasing_concierge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
